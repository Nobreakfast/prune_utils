{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([108.8709, 108.6249, 108.3073,  ...,  19.2459,  19.2225,  18.7691])\n",
      "tensor([1.9698, 1.9577, 1.9563,  ..., 0.3469, 0.3422, 0.3386])\n",
      "tensor([1.9707, 1.9615, 1.9569,  ..., 0.3467, 0.3446, 0.3431])\n",
      "tensor([2.4029, 2.3983, 2.3899,  ..., 0.4255, 0.4195, 0.4146])\n",
      "tensor([2.4085, 2.3986, 2.3917,  ..., 0.4243, 0.4221, 0.4122])\n",
      "tensor([3.4034, 3.3960, 3.3882,  ..., 0.5975, 0.5963, 0.5911])\n",
      "tensor([3.4142, 3.4032, 3.3970,  ..., 0.6015, 0.5951, 0.5892])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2048, 4096)\n",
    "u, s, v = torch.svd(x)\n",
    "print(s)\n",
    "\n",
    "torch.nn.init.xavier_normal_(x, gain=1.0)\n",
    "u, s, v = torch.svd(x)\n",
    "print(s)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(x, gain=1.0)\n",
    "u, s, v = torch.svd(x)\n",
    "print(s)\n",
    "\n",
    "torch.nn.init.kaiming_normal_(x, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "u, s, v = torch.svd(x)\n",
    "print(s)\n",
    "\n",
    "torch.nn.init.kaiming_uniform_(x, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "u, s, v = torch.svd(x)\n",
    "print(s)\n",
    "\n",
    "\n",
    "torch.nn.init.kaiming_normal_(x, a=0, mode='fan_out', nonlinearity='leaky_relu')\n",
    "u, s, v = torch.svd(x)\n",
    "print(s)\n",
    "\n",
    "torch.nn.init.kaiming_uniform_(x, a=0, mode='fan_out', nonlinearity='leaky_relu')\n",
    "u, s, v = torch.svd(x)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as numpy\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) # 3x3 conv\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # batch norm\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # 3x3 conv\n",
    "        self.bn2 = nn.BatchNorm2d(planes) # batch norm\n",
    "\n",
    "        self.shortcut = nn.Sequential() # identity\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes,self.expansion*planes,kernel_size=1,stride=stride,bias=False), # 1x1 conv\n",
    "                nn.BatchNorm2d(self.expansion*planes) # batch norm\n",
    "            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x))) # relu -> batch norm -> conv\n",
    "        out = self.bn2(self.conv2(out)) # batch norm -> conv\n",
    "        out += self.shortcut(x) # identity\n",
    "        out = nn.ReLU()(out) # relu\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False) # 3x3 conv\n",
    "        self.bn1 = nn.BatchNorm2d(64) # batch norm\n",
    "        self.layer1 = self._make_layer(block,64,num_blocks[0],stride=1) # 3x3 conv\n",
    "        self.layer2 = self._make_layer(block,128,num_blocks[1],stride=2) # 3x3 conv\n",
    "        self.layer3 = self._make_layer(block,256,num_blocks[2],stride=2) # 3x3 conv\n",
    "        self.layer4 = self._make_layer(block,512,num_blocks[3],stride=2) # 3x3 conv\n",
    "        self.linear = nn.Linear(512*block.expansion,num_classes) # linear\n",
    "\n",
    "    def _make_layer(self,block,planes,num_blocks,stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1) # [1,1,1,1] for stride=1 or [2,1,1,1] for stride=2\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes,planes,stride)) # append basic block\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers) # sequential\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x))) # relu -> batch norm -> conv\n",
    "        out = self.layer1(out) # basic block\n",
    "        out = self.layer2(out) # basic block\n",
    "        out = self.layer3(out) # basic block\n",
    "        out = self.layer4(out) # basic block\n",
    "        out = nn.AvgPool2d(4)(out) # average pooling\n",
    "        out = out.view(out.size(0),-1) # flatten\n",
    "        out = self.linear(out) # linear\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock,[2,2,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as numpy\n",
    "\n",
    "class BasicBlockNoRes(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlockNoRes,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) # 3x3 conv\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # batch norm\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # 3x3 conv\n",
    "        self.bn2 = nn.BatchNorm2d(planes) # batch norm\n",
    "\n",
    "        self.shortcut = nn.Sequential() # identity\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes,self.expansion*planes,kernel_size=1,stride=stride,bias=False), # 1x1 conv\n",
    "                nn.BatchNorm2d(self.expansion*planes) # batch norm\n",
    "            )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x))) # relu -> batch norm -> conv\n",
    "        out = self.bn2(self.conv2(out)) # batch norm -> conv\n",
    "        # out += self.shortcut(x) # identity\n",
    "        out = nn.ReLU()(out) # relu\n",
    "        return out\n",
    "\n",
    "def ResNet18nores():\n",
    "    return ResNet(BasicBlockNoRes,[2,2,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNet18()\n",
    "model_nores = ResNet18nores()\n",
    "times = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.3784, grad_fn=<DivBackward0>) tensor(114.5417, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "max_lip = 0\n",
    "max_lip_nr = 0\n",
    "for i in trange(times):\n",
    "    input = torch.randn(4,3,32,32)\n",
    "    eps = 1e-7*torch.randn(4,3,32,32)\n",
    "    p = 1\n",
    "\n",
    "    x1 = input\n",
    "    x2 = input + eps\n",
    "\n",
    "    y1 = model(x1)\n",
    "    y2 = model(x2)\n",
    "\n",
    "    y1_nr = model_nores(x1)\n",
    "    y2_nr = model_nores(x2)\n",
    "\n",
    "    # lip = torch.max(torch.norm(y1-y2,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    # lip_nr = torch.max(torch.norm(y1_nr-y2_nr,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    lip = torch.max(torch.sqrt(torch.sum(torch.pow(y1-y2,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "    lip_nr = torch.max(torch.sqrt(torch.sum(torch.pow(y1_nr-y2_nr,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "\n",
    "    if lip > max_lip:\n",
    "        max_lip = lip\n",
    "    if lip_nr > max_lip_nr:\n",
    "        max_lip_nr = lip_nr\n",
    "\n",
    "print(max_lip, max_lip_nr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(76.4904, grad_fn=<DivBackward0>) tensor(108.6592, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# xavier_normal the model and model_nores\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    # if isinstance(m, nn.Linear):\n",
    "    #     torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "for m in model_nores.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    # if isinstance(m, nn.Linear):\n",
    "    #     torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "max_lip = 0\n",
    "max_lip_nr = 0\n",
    "for i in trange(times):\n",
    "    input = torch.randn(4,3,32,32)\n",
    "    eps = 1e-7*torch.randn(4,3,32,32)\n",
    "    p = 1\n",
    "\n",
    "    x1 = input\n",
    "    x2 = input + eps\n",
    "\n",
    "    y1 = model(x1)\n",
    "    y2 = model(x2)\n",
    "\n",
    "    y1_nr = model_nores(x1)\n",
    "    y2_nr = model_nores(x2)\n",
    "\n",
    "    # lip = torch.max(torch.norm(y1-y2,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    # lip_nr = torch.max(torch.norm(y1_nr-y2_nr,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    lip = torch.max(torch.sqrt(torch.sum(torch.pow(y1-y2,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "    lip_nr = torch.max(torch.sqrt(torch.sum(torch.pow(y1_nr-y2_nr,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "\n",
    "    if lip > max_lip:\n",
    "        max_lip = lip\n",
    "    if lip_nr > max_lip_nr:\n",
    "        max_lip_nr = lip_nr\n",
    "\n",
    "print(max_lip, max_lip_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(76.7082, grad_fn=<DivBackward0>) tensor(116.1856, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# kaiming_normal the model and model_nores\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "    # if isinstance(m, nn.Linear):\n",
    "    #     torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "for m in model_nores.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "    # if isinstance(m, nn.Linear):\n",
    "    #     torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "max_lip = 0\n",
    "max_lip_nr = 0\n",
    "for i in trange(times):\n",
    "    input = torch.randn(4,3,32,32)\n",
    "    eps = 1e-7*torch.randn(4,3,32,32)\n",
    "    p = 1\n",
    "\n",
    "    x1 = input\n",
    "    x2 = input + eps\n",
    "\n",
    "    y1 = model(x1)\n",
    "    y2 = model(x2)\n",
    "\n",
    "    y1_nr = model_nores(x1)\n",
    "    y2_nr = model_nores(x2)\n",
    "\n",
    "    # lip = torch.max(torch.norm(y1-y2,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    # lip_nr = torch.max(torch.norm(y1_nr-y2_nr,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    lip = torch.max(torch.sqrt(torch.sum(torch.pow(y1-y2,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "    lip_nr = torch.max(torch.sqrt(torch.sum(torch.pow(y1_nr-y2_nr,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "\n",
    "    if lip > max_lip:\n",
    "        max_lip = lip\n",
    "    if lip_nr > max_lip_nr:\n",
    "        max_lip_nr = lip_nr\n",
    "\n",
    "print(max_lip, max_lip_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(80.1550, grad_fn=<DivBackward0>) tensor(106.0311, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# kaiming_normal the model and model_nores\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out', nonlinearity='relu')\n",
    "    # if isinstance(m, nn.Linear):\n",
    "    #     torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "for m in model_nores.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out', nonlinearity='relu')\n",
    "    # if isinstance(m, nn.Linear):\n",
    "    #     torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "max_lip = 0\n",
    "max_lip_nr = 0\n",
    "for i in trange(times):\n",
    "    input = torch.randn(4,3,32,32)\n",
    "    eps = 1e-7*torch.randn(4,3,32,32)\n",
    "    p = 1\n",
    "\n",
    "    x1 = input\n",
    "    x2 = input + eps\n",
    "\n",
    "    y1 = model(x1)\n",
    "    y2 = model(x2)\n",
    "\n",
    "    y1_nr = model_nores(x1)\n",
    "    y2_nr = model_nores(x2)\n",
    "\n",
    "    # lip = torch.max(torch.norm(y1-y2,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    # lip_nr = torch.max(torch.norm(y1_nr-y2_nr,p=p,dim=1))/torch.max(torch.norm(x1-x2,p=p,dim=1))\n",
    "    lip = torch.max(torch.sqrt(torch.sum(torch.pow(y1-y2,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "    lip_nr = torch.max(torch.sqrt(torch.sum(torch.pow(y1_nr-y2_nr,2),dim=1)))/torch.max(torch.sqrt(torch.sum(torch.pow(x1-x2,2),dim=1)))\n",
    "\n",
    "    if lip > max_lip:\n",
    "        max_lip = lip\n",
    "    if lip_nr > max_lip_nr:\n",
    "        max_lip_nr = lip_nr\n",
    "\n",
    "print(max_lip, max_lip_nr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
